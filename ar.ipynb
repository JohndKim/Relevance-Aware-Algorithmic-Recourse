{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import load_dataset\n",
    "import numpy as np\n",
    "from iron import phi\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from dataloader import load_dataset\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################   FOR GETTING FIRST TIME RESULTS   ##################\n",
    "def get_algorithmic_recourse_results(name, model, step, is_global, multiplier):\n",
    "    '''\n",
    "    Args:\n",
    "    - name (str)            : dataset name\n",
    "    - model (str)           : model to run\n",
    "    - step (int)            : data point frequency usae\n",
    "    - is_global (bool)      : if we are optimizing for the maximum we can get or a specific target\n",
    "    - multiplier (float)    : target multiplier, only if we are looking for a specific taret\n",
    "\n",
    "    update csv with new structure\n",
    "    '''\n",
    "    # PRE PROCESSING DATA AND GETTING DATA\n",
    "    X_train, y_train, X_test, y_test, features, ph, phis, reg = pre_process_data(name, model)\n",
    "\n",
    "    print('starting baysopt')\n",
    "    # getting results from bayesian optimization\n",
    "    abs_results, pdiffs_abs_y, params, pred_y_abs = run_all_abs_y(features, reg, X_test, y_test, step, is_global, multiplier)\n",
    "    rel_results, pdiffs_abs_rel, params_rel, pred_y_rel = run_all_abs_rel(features, reg, X_test, y_train, y_test, step, phis, ph, is_global, multiplier)\n",
    "\n",
    "    # getting the iterations of each result (the value predicted from each iteration of the bayesopt model, so 55 for each)\n",
    "    absolute_iterations = get_iterations(pred_y_abs)\n",
    "    relevance_iterations = get_iterations(pred_y_rel)\n",
    "\n",
    "    # p1, p2, f1, f2, r1, r2 = get_paths(name, model)\n",
    "    pdiffs_abs_file, pdiffs_rel_file, f_abs_file, f_rel_file, r_abs_file, r_rel_file = get_paths(name, model, is_global, multiplier)\n",
    "\n",
    "    # saves data into csv\n",
    "    all_dfs_to_csvs(pdiffs_abs_file, pdiffs_rel_file, f_abs_file, f_rel_file, r_abs_file, r_rel_file, pdiffs_abs_y, pdiffs_abs_rel, params, params_rel, absolute_iterations, relevance_iterations)\n",
    "\n",
    "    # retrieves differences and features from the files we created earlier\n",
    "    pdiffs_abs = read_file(pdiffs_abs_file)\n",
    "    pdiffs_rel = read_file(pdiffs_rel_file)\n",
    "    features_abs = read_file(f_abs_file)\n",
    "    features_rel = read_file(f_rel_file)\n",
    "\n",
    "    # changes features to ensure order?\n",
    "    new_f_abs = change_dict(read_dict_file(f_abs_file), features)\n",
    "    new_f_rel = change_dict(read_dict_file(f_rel_file), features)\n",
    "    features_abs = df_to_csv(new_f_abs, f_abs_file)\n",
    "    features_rel = df_to_csv(new_f_rel, f_rel_file)\n",
    "    \n",
    "    # features_abs, features_rel = convert_new_df(new_f_abs, new_f_rel, f_abs_file, f_rel_file)\n",
    "    features_abs = read_file(f_abs_file)\n",
    "    features_rel = read_file(f_rel_file)\n",
    "\n",
    "################## 1. DATA PRE PROCESSING ##################\n",
    "\n",
    "def pre_process_data(name, model):\n",
    "    '''\n",
    "    Args:\n",
    "    - name (str)            : dataset name\n",
    "    - model (str)           : model to run\n",
    "    - step (int)            : data point frequency usae\n",
    "\n",
    "    returns X/y test/train data, list of feature names, global relevance functions, and regressor\n",
    "    '''\n",
    "    X_train, y_train, X_test, y_test = get_data(name)\n",
    "\n",
    "    # FOR ONE OF THE DATASETS, THIS RENAMES COLUMNS TO BE USABLE BY BAYESIAN OPTIMIZATION\n",
    "    X_train = X_train.rename(columns = {\"fixed.acidity\": \"fixed_acidity\",\t\"volatile.acidity\": \"volatile_acidity\",\t\"citric.acid\": \"citric_acid\", \"residual.sugar\": \"residual_sugar\", \"free.sulfur.dioxide\": \"free_sulfur_dioxide\", \"total.sulfur.dioxide\": \"total_sulfur_dioxide\"})\n",
    "    X_test = X_test.rename(columns = {\"fixed.acidity\": \"fixed_acidity\",\t\"volatile.acidity\": \"volatile_acidity\",\t\"citric.acid\": \"citric_acid\", \"residual.sugar\": \"residual_sugar\", \"free.sulfur.dioxide\": \"free_sulfur_dioxide\", \"total.sulfur.dioxide\": \"total_sulfur_dioxide\"})\n",
    "\n",
    "    print('received data')\n",
    "    features = list(X_test.columns.values)\n",
    "    print(\", \".join(X_train.columns))\n",
    "    ph, phis = get_phis_global(y_train, y_test)\n",
    "    print(\"received phi/phis\")\n",
    "    reg = create_regressor(X_train, y_train, model)\n",
    "    print('received regressor')\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, features, ph, phis, reg\n",
    "\n",
    "def get_data(name):\n",
    "    '''\n",
    "    Args:\n",
    "    - name (str)            : dataset name\n",
    "\n",
    "    pre-process data (scale, train test split)\n",
    "    return X_train, X_test, y_train, Y_test\n",
    "    '''\n",
    "    # get train test split\n",
    "    if file_is_of_dataset(name): X_train, y_train, X_test, y_test = load_dataset(name)\n",
    "    else: X_train, y_train, X_test, y_test = manually_label_categorical_data(name)\n",
    "\n",
    "    # one hot encode\n",
    "    X_train = encode_categorical_columns(X_train)\n",
    "    X_test = encode_categorical_columns(X_test)\n",
    "\n",
    "    # fit scaler to X_train\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # fit transform X_train and X_test\n",
    "    scaled_X_train = scaler.transform(X_train)\n",
    "    scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "    X_train = pd.DataFrame(scaled_X_train, index=X_train.index, columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(scaled_X_test, index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "    # print(\"X TRAIN AFTER SCALED\")\n",
    "    # print(X_train)\n",
    "\n",
    "    y_test = y_test.astype(float)\n",
    "    y_train = y_train.astype(float)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def encode_categorical_columns(df):\n",
    "    '''\n",
    "    Args:\n",
    "    - df (df)            : dataframe to encode\n",
    "\n",
    "    encodes categorical columns through on hot encoder\n",
    "    return encoded df\n",
    "    '''\n",
    "    \n",
    "    # get categorical column names\n",
    "    categorical_columns = df.select_dtypes(include='category').columns.tolist()\n",
    "\n",
    "    # initialize OneHotEncoder\n",
    "    # encoder = OneHotEncoder(sparse_output=False)  # Setting drop='first' to avoid dummy variable trap # this is deprecated\n",
    "    encoder = OneHotEncoder(sparse=False)  # Setting drop='first' to avoid dummy variable trap\n",
    "\n",
    "    # Fit and transform on categorical columns\n",
    "    encoded_data = encoder.fit_transform(df[categorical_columns])\n",
    "\n",
    "    # Create column names for new features\n",
    "    new_columns = encoder.get_feature_names_out(categorical_columns)\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=new_columns, index=df.index)\n",
    "\n",
    "    # Concatenate with original DataFrame, dropping original categorical columns\n",
    "    df_encoded = pd.concat([df.drop(columns=categorical_columns), encoded_df], axis=1)\n",
    "\n",
    "    return df_encoded\n",
    "\n",
    "def manually_label_categorical_data(name):\n",
    "    '''\n",
    "    Args:\n",
    "    - name (str) : name of dataset to use\n",
    "    \n",
    "    for manually one hot encoding if file is not part of the provided datasets \n",
    "    \n",
    "    returns x/y train/test split encoded data\n",
    "    '''\n",
    "    p = path + f'all_datasets/{name}.csv'\n",
    "    df = pd.read_csv(p)\n",
    "\n",
    "    y = df['charges']  # Get the target column\n",
    "    X = df.drop(columns=['charges'])  # Drop the targetcolumn to get features\n",
    "\n",
    "    # set columns as categorical\n",
    "    X['sex'] = X['sex'].astype('category')\n",
    "    X['children'] = X['children'].astype('category')\n",
    "    X['smoker'] = X['smoker'].astype('category')\n",
    "    X['region'] = X['region'].astype('category')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    y_train = y_train.values.astype(float)\n",
    "    y_test = y_test.values.astype(float)\n",
    "\n",
    "    print(X_train.columns)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def get_phis_local(y_train, y_test, y_min, y_max, y_orig, y_target):\n",
    "    '''\n",
    "    Args:\n",
    "    - y_train (array)     : y train\n",
    "    - y_test (array)      : y test\n",
    "    - y_min (float)       : minimum y\n",
    "    - y_max (float)       : max y\n",
    "    - y_orig (float)      : original y\n",
    "    - y_target (float)    : target y we optimize for (relevance = 1)\n",
    "    Returns, ph, phis\n",
    "\n",
    "\n",
    "    Returns relevance function for a specific point we want to get\n",
    "    and the according phis for the test dataset\n",
    "    '''\n",
    "    pts = [y_min, 0, 0,\n",
    "    y_orig, 0.5, 0,\n",
    "    y_target, 1, 0,\n",
    "    y_max, 0, 0]\n",
    "\n",
    "    if y_orig == y_min:\n",
    "        pts = [y_orig, 0.5, 0,\n",
    "        y_target, 1, 0,\n",
    "        y_max, 0, 0]\n",
    "    if y_orig == y_max:\n",
    "        pts = [y_min, 0, 0,\n",
    "        y_orig, 0.5, 0,\n",
    "        y_target, 1, 0]\n",
    "    if y_orig == 0:\n",
    "        pts = [y_min, 0.5, 0,\n",
    "        y_orig, 1, 0,\n",
    "        y_max, 0, 0]\n",
    "    if y_orig == y_min and y_orig == 0:\n",
    "        pts = [y_target, 1, 0,\n",
    "        y_max, 0, 0]\n",
    "    if y_orig == y_max and y_orig == 0:\n",
    "        pts = [y_min, 0.5, 0,\n",
    "        y_orig, 1, 0]\n",
    "\n",
    "    # print(f\"points: {pts}\")\n",
    "\n",
    "    npts = int(len(pts) / 3)\n",
    "\n",
    "\n",
    "    # positive change\n",
    "    control_pts = {\n",
    "    \"control_pts\": pts,\n",
    "    \"npts\": npts,\n",
    "    }\n",
    "    # print(f\"ctrl_pts : {control_pts}\")\n",
    "\n",
    "    ph = phi.phi_control(y_train, control_pts=control_pts, method=\"range\")\n",
    "    phis = phi.phi(pd.Series(y_test), phi_parms=ph)\n",
    "\n",
    "    # print(f'ph: {ph}')\n",
    "    # print(f'phis: {phis}')\n",
    "\n",
    "    return ph, phis\n",
    "\n",
    "def get_phis_global(y_train, y_test):\n",
    "    '''\n",
    "    Args:\n",
    "    - y_train (array)     : y train\n",
    "    - y_test (array)      : y test\n",
    "\n",
    "    returns global relevance functions (towards right extreme)\n",
    "    '''\n",
    "    # solve something at the dataset level\n",
    "    ph = phi.phi_control(pd.Series(y_train), extr_type=\"high\")\n",
    "    # relevance (phis) on test\n",
    "    phis = phi.phi(pd.Series(y_test), phi_parms=ph)\n",
    "\n",
    "    return ph, phis\n",
    "\n",
    "def create_regressor(X_train, y_train, model):\n",
    "    '''\n",
    "    Args:\n",
    "    - X_train (array)       : X train data\n",
    "    - y_train (array)       : y train data\n",
    "    - model (str)           : model name we want to run\n",
    "\n",
    "    Returns regression model based on the data and type\n",
    "    '''\n",
    "\n",
    "    if model == \"rf\": reg = RandomForestRegressor(n_estimators=500, random_state = 0) # n = 500\n",
    "    else: reg = LGBMRegressor(n_estimators=500, random_state = 0)\n",
    "\n",
    "    reg.fit(X_train.values, y_train)\n",
    "\n",
    "    return reg\n",
    "\n",
    "\n",
    "################## 2. RUNNING BAYESOPT ##################\n",
    "def run_all_abs_y(features, reg, X_test, y_test, step, is_global, multiplier):\n",
    "    '''\n",
    "    Args:\n",
    "    - features (array)      : list of features\n",
    "    - reg (model)           : regression model\n",
    "    - X_test (array)        : X test data\n",
    "    - y_test (array)        : y test data\n",
    "    - step (int)            : data point frequency\n",
    "    - is_global (bool)      : chooses if we want to run entire data or not\n",
    "    - multiplier (float)    : target we optimize for\n",
    "    \n",
    "    NOTE: MUST REPLACE THE FEATURES IN THE BLACK_BOX FUNCTION AND X 2D ARRAY WITH FEATURES YOU WANT TO RUN\n",
    "\n",
    "    runs bayesian optimization for algorithmic recourse\n",
    "\n",
    "    returns abs_results, pdiffs_abs_y, params, pred_ys\n",
    "    '''\n",
    "    print(\"start abs bayesopt\")\n",
    "    print(features)\n",
    "\n",
    "    pdiffs_abs_y = []\n",
    "    pred_ys = []\n",
    "    params = []\n",
    "    # pred_y_rel = []\n",
    "    # rel = []\n",
    "    abs_results = []\n",
    "\n",
    "    base = []\n",
    "    pbounds = {} # 5%\n",
    "\n",
    "    p_mult = 1 + multiplier\n",
    "    n_mult = 1 - multiplier\n",
    "\n",
    "\n",
    "    # FOR CREATING PBOUNDS + STARTING X (base)\n",
    "    # for each row/feature in our dataset\n",
    "    for i in range(0, len(X_test), step):\n",
    "        predicted_ys = []\n",
    "        y_orig = y_test[i] # get true y\n",
    "\n",
    "\n",
    "        if not is_global:\n",
    "            if y_test[i] < 0: mult = n_mult\n",
    "            else: mult = p_mult\n",
    "            y_target = y_test[i] * mult # target val we want to reach\n",
    "\n",
    "            y_diff = abs(y_target - y_test[i]) # distance(difference) from that target value\n",
    "\n",
    "        # creating pbounds based on mean of features\n",
    "        for f in features:\n",
    "            x = X_test.iloc[i][f]\n",
    "            base.append(x)\n",
    "            if is_global: sd = 0.05 # \n",
    "            else: sd = 1.0          #\n",
    "            pbounds[f] = (min(x - (sd * x), x + (sd * x)), max(x - (sd * x), x + (sd * x)))\n",
    "\n",
    "        # optimizes for maximum y difference\n",
    "        def black_box_function_abs_y(\n",
    "\n",
    "\n",
    "\n",
    "age, bmi, sex_female, sex_male, children_0, children_1, children_2, children_3, children_4, children_5, smoker_no, smoker_yes, region_northeast, region_northwest, region_southeast, region_southwest\n",
    "\n",
    "\n",
    "        ):\n",
    "\n",
    "            x = [[\n",
    "\n",
    "\n",
    "\n",
    "age, bmi, sex_female, sex_male, children_0, children_1, children_2, children_3, children_4, children_5, smoker_no, smoker_yes, region_northeast, region_northwest, region_southeast, region_southwest\n",
    "\n",
    "\n",
    "            ]]\n",
    "\n",
    "            y1 = reg.predict(x)     # predicted y value\n",
    "            predicted_ys.append(y1[0])\n",
    "\n",
    "            if is_global:\n",
    "                if y_orig != 0: pdiff = ((y1[0] - y_orig) / abs(y_orig)) * 100  # difference between original and predicted y\n",
    "                else: pdiff = 0\n",
    "            else:\n",
    "                if y_diff == 0: pdiff = 0\n",
    "                else:\n",
    "                    pdiff = -abs(y1[0] - y_target) / y_diff\n",
    "\n",
    "            return pdiff\n",
    "\n",
    "        ########### CREATING BAYESIAN OPTIMIZER ###########\n",
    "        optimizer_abs_y = BayesianOptimization(\n",
    "            f=black_box_function_abs_y,  # f(x)\n",
    "            pbounds=pbounds,  # parameter bounds\n",
    "            random_state=1,\n",
    "            allow_duplicate_points=True,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        ########### MAXIMIZING (finding optimums) ###########\n",
    "        optimizer_abs_y.maximize(\n",
    "            init_points=5,  # num of random steps to perform --> can help diversify explore space\n",
    "            n_iter=50,  # number of iterations/steps of baye opt --> more = more accurate optimum\n",
    "        )\n",
    "\n",
    "\n",
    "        opt = optimizer_abs_y.max['params']                 # features for the best case\n",
    "        pdiffs_abs_y.append(optimizer_abs_y.max['target'])  # y difference for best case\n",
    "        params.append(opt)\n",
    "        abs_results.append(optimizer_abs_y.res)\n",
    "\n",
    "        # iteration = []\n",
    "        # for it in optimizer_abs_y.res:\n",
    "        #     # params for the best case\n",
    "        #     param = []\n",
    "        #     for val in it['params'].values(): param.append(val)\n",
    "        #     # optimized prediction for this case\n",
    "        #     pred = reg.predict([param])[0]\n",
    "        #     iteration.append(pred)\n",
    "\n",
    "        # abs_results.append(iteration)\n",
    "\n",
    "        pred_ys.append(predicted_ys)\n",
    "\n",
    "    print(\"end abs bayesopt\")\n",
    "\n",
    "\n",
    "    return abs_results, pdiffs_abs_y, params, pred_ys\n",
    "\n",
    "def run_all_abs_rel(features, reg, X_test, y_train, y_test, step, phis, ph, is_global, multiplier):\n",
    "    '''\n",
    "    Args:\n",
    "    - features (array)      : list of features\n",
    "    - reg (model)           : regression model\n",
    "    - X_test (array)        : X test data\n",
    "    - y_test (array)        : y test data\n",
    "    - step (int)            : data point frequency\n",
    "    - phis (array)          : relevance of each y\n",
    "    - ph (ph)               : relevance function\n",
    "    - is_global (bool)      : chooses if we want to run entire data or not\n",
    "    - multiplier (float)    : target we optimize for\n",
    "    \n",
    "    NOTE: MUST REPLACE THE FEATURES IN THE BLACK_BOX FUNCTION AND X 2D ARRAY WITH FEATURES YOU WANT TO RUN\n",
    "\n",
    "    runs bayesian optimization based on the target\n",
    "\n",
    "    returns rel_results, pdiffs_abs_rel, params_rel, pred_ys\n",
    "    \n",
    "    '''\n",
    "    print(\"start rel bayesopt\")\n",
    "\n",
    "    pdiffs_abs_rel = []\n",
    "    params_rel = []\n",
    "    pred_ys = []\n",
    "    rel_results = []\n",
    "\n",
    "    base = []\n",
    "    pbounds = {}\n",
    "\n",
    "    y_min = min(y_test)\n",
    "    y_max = max(y_test)\n",
    "\n",
    "\n",
    "    p_mult = 1 + multiplier\n",
    "    n_mult = 1 - multiplier\n",
    "\n",
    "    # CREATES PBOUNDS AND BASE X VALUES\n",
    "    for i in range(0, len(X_test), step):\n",
    "        predicted_ys = []\n",
    "\n",
    "        y_rel_global = phis[i]\n",
    "\n",
    "        # for a local/specific point we want to get\n",
    "        if not is_global:\n",
    "            if y_test[i] < 0: mult = n_mult\n",
    "            else: mult = p_mult\n",
    "            # TARGET Y WE WANT TO GET\n",
    "            y_target = y_test[i] * mult\n",
    "            # relevance of all y according to the specific relevance function we created\n",
    "            ph, phis = get_phis_local(y_train, y_test, y_min, y_max, y_test[i], y_target)\n",
    "            # true relevances for all y_test w/ our specific relevance function created for every y_test\n",
    "            # y_rel_diff = abs(1 - phis[i])\n",
    "            # print(f\"y_rel_diff = {y_rel_diff}, y_rel_orig = {phis[i]}, y_rel_target = {phi.phi(pd.Series(y_target), phi_parms=ph)}\")\n",
    "\n",
    "        y_rel = phis[i]                             # true relevance of the y\n",
    "            # relevance difference between target relevance and\n",
    "\n",
    "\n",
    "\n",
    "        # creating pbounds based on mean of features\n",
    "        for f in features:\n",
    "            x = X_test.iloc[i][f]\n",
    "            base.append(x)\n",
    "            if is_global: sd = 0.05\n",
    "            else: sd  = 1.0\n",
    "            pbounds[f] = (min(x - (sd * x), x + (sd * x)), max(x - (sd * x), x + (sd * x)))\n",
    "\n",
    "\n",
    "        def black_box_function_abs_rel(\n",
    "\n",
    "\n",
    "age, bmi, sex_female, sex_male, children_0, children_1, children_2, children_3, children_4, children_5, smoker_no, smoker_yes, region_northeast, region_northwest, region_southeast, region_southwest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "):\n",
    "            # when generating x, we will have a list of vars with all the numeric and categorical\n",
    "            x = [[\n",
    "\n",
    "age, bmi, sex_female, sex_male, children_0, children_1, children_2, children_3, children_4, children_5, smoker_no, smoker_yes, region_northeast, region_northwest, region_southeast, region_southwest\n",
    "\n",
    "\n",
    "]]\n",
    "\n",
    "            y1 = reg.predict(x)                                         # predicted y\n",
    "            y_rel_pred = phi.phi(pd.Series(y1[0]), phi_parms=ph)        # relevance of the predicted y\n",
    "            if is_global:\n",
    "                pdiff = y_rel_pred[0]\n",
    "                # if y_rel != 0: pdiff = ((y_rel_pred[0] - y_rel)/abs(y_rel)) * 100                 # optimize for better positive relevance change\n",
    "                # else: pdiff = 0\n",
    "            # closer our predicted rel is, the better\n",
    "            else: pdiff = y_rel_pred[0]                                 # optimize for the better y\n",
    "\n",
    "            predicted_ys.append(y1[0])\n",
    "\n",
    "            return pdiff\n",
    "\n",
    "        ########### CREATING BAYESIAN OPTIMIZER ###########\n",
    "        optimizer_abs_rel = BayesianOptimization(\n",
    "            f=black_box_function_abs_rel,  # f(x)\n",
    "            pbounds=pbounds,  # parameter bounds\n",
    "            random_state=1,\n",
    "            allow_duplicate_points=True,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        ########### MAXIMIZING (finding optimums) ###########\n",
    "        optimizer_abs_rel.maximize(\n",
    "            init_points=5,  # num of random steps to perform --> can help diversify explore space\n",
    "            n_iter=50,  # number of iterations/steps of baye opt --> more = more accurate optimum\n",
    "        )\n",
    "\n",
    "        opt = optimizer_abs_rel.max['params']                           # features for the best case\n",
    "        pdiffs_abs_rel.append(optimizer_abs_rel.max['target'])          # relevance difference for the best case\n",
    "        params_rel.append(opt)\n",
    "        rel_results.append(optimizer_abs_rel.res)\n",
    "\n",
    "\n",
    "        # iteration = []\n",
    "        # for it in optimizer_abs_rel.res:\n",
    "        #     # params for the best case\n",
    "        #     param = []\n",
    "        #     for val in it['params'].values(): param.append(val)\n",
    "        #     # optimized prediction for this case\n",
    "        #     pred = reg.predict([param])[0]\n",
    "        #     iteration.append(pred)\n",
    "\n",
    "        # rel_results.append(iteration)\n",
    "\n",
    "        pred_ys.append(predicted_ys)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"end rel bayesopt\")\n",
    "\n",
    "    return rel_results, pdiffs_abs_rel, params_rel, pred_ys\n",
    "\n",
    "################## 3. ANALYZING DATA AND STORING THEM ##################\n",
    "\n",
    "# get the iterations from each bayesopt run\n",
    "def get_iterations(results):\n",
    "    return results\n",
    "\n",
    "def get_iterations_till_target(y_test, params, iterations, reg, step, multiplier):\n",
    "    '''\n",
    "    Args:\n",
    "    - y_test (array)        : y test data\n",
    "    - params                : parameters for the model\n",
    "    - iterations            : iterations of the model\n",
    "    - reg (model)           : regression model\n",
    "    - step (int)            : data point frequency\n",
    "    - multiplier (float)    : target we optimize for\n",
    "\n",
    "    finds the number of iterations to get the best value\n",
    "\n",
    "    returns iterations_till_target, err\n",
    "    '''\n",
    "    \n",
    "    iterations_till_target = []\n",
    "    p_mult = 1 + multiplier\n",
    "    n_mult = 1 - multiplier\n",
    "\n",
    "    # loops over iterations (it = array of iterations of one case)\n",
    "    for i, it in enumerate(iterations):\n",
    "        # getting optimized (predicted) y\n",
    "        if i == 0: continue # skip header\n",
    "        i -= 1\n",
    "        x_param = [params[i]]\n",
    "        y_pred = reg.predict(x_param)[0]\n",
    "\n",
    "        # getting target y\n",
    "        if y_test[i * step] < 0: mult = n_mult\n",
    "        else: mult = p_mult\n",
    "        y_target = y_test[i * step] * mult\n",
    "\n",
    "        added = False\n",
    "\n",
    "        # index, predicted y for that iteration (1-50)\n",
    "        for j, pred in enumerate(it, 1):\n",
    "            if added: break\n",
    "            if pred != y_pred: continue\n",
    "\n",
    "            # if this is the case we got the prediction, add to list\n",
    "            # iterations_till_target.append((j, pred, y_target))\n",
    "            iterations_till_target.append(j)\n",
    "            added = True\n",
    "\n",
    "\n",
    "        if not added: iterations_till_target.append(-1000000)\n",
    "\n",
    "    if -1000000 in iterations_till_target: err = 0\n",
    "    else: err = 1\n",
    "    return iterations_till_target, err\n",
    "\n",
    "def get_paths(name, model, is_global, multiplier):\n",
    "    '''\n",
    "    Args:\n",
    "    - name (str)            : dataset name\n",
    "    - model (str)           : model to run\n",
    "    - is_global (bool)      : if we are optimizing for the maximum we can get or a specific target\n",
    "    - multiplier (float)    : target multiplier\n",
    "\n",
    "    get paths to store the results in\n",
    "    '''\n",
    "    if is_global:\n",
    "        # suffix = \"_global\"\n",
    "        suffix = \"\"\n",
    "        prefix = \"global_\"\n",
    "\n",
    "    else:\n",
    "        suffix = \"\"\n",
    "        prefix = \"specific-\" + str(multiplier) + \"-\"\n",
    "\n",
    "\n",
    "    # path = f\"/Users/johnkim/Developer/algorithmic recourse/all_datasets/\"\n",
    "    # poop\n",
    "    pdiff_path = path + prefix + \"results/pdiffs/\"\n",
    "    feat_path = path + prefix + \"results/features/\"\n",
    "    iter_path = path + prefix + \"results/iterations/\"\n",
    "    if not os.path.exists(pdiff_path): os.makedirs(pdiff_path)\n",
    "    if not os.path.exists(feat_path): os.makedirs(feat_path)\n",
    "    if not os.path.exists(iter_path): os.makedirs(iter_path)\n",
    "        \n",
    "    p1 = pdiff_path + name + \"_\" + model + suffix + \"_abs.csv\"\n",
    "    p2 = pdiff_path + name + \"_\" + model + suffix + \"_rel.csv\"\n",
    "    f1 = feat_path + name + \"_\" + model + suffix + \"_abs.csv\"\n",
    "    f2 = feat_path + name + \"_\" + model + suffix + \"_rel.csv\"\n",
    "    r1 = iter_path + name + \"_\" + model + suffix + \"_abs.csv\"\n",
    "    r2 = iter_path + name + \"_\" + model + suffix + \"_rel.csv\"\n",
    "\n",
    "        # p1 = \"/Users/johnkim/Developer/algorithmic recourse/\" + prefix + \"csv/pdiffs_\" + name + \"_\" + model + suffix + \"_abs_new.csv\"\n",
    "        # p2 = \"/Users/johnkim/Developer/algorithmic recourse/\" + prefix + \"csv/pdiffs_\" + name + \"_\" + model + suffix + \"_rel_new.csv\"\n",
    "        # f1 = \"/Users/johnkim/Developer/algorithmic recourse/\" + prefix + \"csv/f_\" + name + \"_\" + model + suffix + \"_abs_new.csv\"\n",
    "        # f2 = \"/Users/johnkim/Developer/algorithmic recourse/\" + prefix + \"csv/f_\" + name + \"_\" + model + suffix + \"_rel_new.csv\"\n",
    "        # r1 = \"/Users/johnkim/Developer/algorithmic recourse/\" + prefix + \"csv/r_\" + name + \"_\" + model + suffix + \"_abs.csv\"\n",
    "        # r2 = \"/Users/johnkim/Developer/algorithmic recourse/\" + prefix + \"csv/r_\" + name + \"_\" + model + suffix + \"_rel.csv\"\n",
    "\n",
    "\n",
    "    return p1, p2, f1, f2, r1, r2\n",
    "\n",
    "# save dataframe into a csv\n",
    "def df_to_csv(dataset, path):\n",
    "    '''\n",
    "    Args:\n",
    "    - dataset (df)          : dataframe \n",
    "    - path (str)            : path to data\n",
    "\n",
    "    store df into a csv at the path\n",
    "    '''\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df.to_csv(path)\n",
    "\n",
    "    return df\n",
    "\n",
    "# saves dfs into csvs\n",
    "def all_dfs_to_csvs(p1, p2, f1, f2, r1, r2, pdiffs_abs_y, pdiffs_abs_rel, params, params_rel, absolute_iterations, relevance_iterations):\n",
    "    '''\n",
    "    Args:\n",
    "    - p1, p2, f1, f2, r1, r2 (str)  : path names\n",
    "    - pdiffs_abs_y, pdiffs_abs_rel, params, params_rel, absolute_iterations, relevance_iterations   : dataframes of our results\n",
    "\n",
    "    save all results to a csv\n",
    "    '''\n",
    "   # return df_to_csv(p1, pdiffs_abs_y), df_to_csv(p2, pdiffs_abs_rel), df_to_csv(f1, params), df_to_csv(f2, params_rel), df_to_csv(r1, absolute_iterations), df_to_csv(r2, relevance_iterations)\n",
    "    return df_to_csv(pdiffs_abs_y, p1), df_to_csv(pdiffs_abs_rel, p2), df_to_csv(params, f1), df_to_csv(params_rel, f2), df_to_csv(absolute_iterations, r1), df_to_csv(relevance_iterations, r2)\n",
    "\n",
    "def read_file(file):\n",
    "    '''\n",
    "    Args:\n",
    "    - file (file)            : dataset name\n",
    "\n",
    "    read file\n",
    "    '''\n",
    "    arr = []\n",
    "    with open(file, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for lines in reader:\n",
    "            if (len(lines) == 2): arr.append(float(lines[1]))\n",
    "            else:\n",
    "                try:\n",
    "                    arr.append(list(np.float64(lines[1:])))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "\n",
    "        if arr[0] == 0: arr.pop(0)\n",
    "\n",
    "\n",
    "    return arr\n",
    "\n",
    "def read_dict_file(file):\n",
    "    '''\n",
    "    Args:\n",
    "    - file (str)            : filename\n",
    "\n",
    "    open and read file dict\n",
    "    '''\n",
    "    arr = []\n",
    "    with open(file, 'r') as f:\n",
    "        dict_reader = csv.DictReader(f)\n",
    "        for line in dict_reader:\n",
    "            line.pop('', None)\n",
    "            arr.append(line)\n",
    "\n",
    "    return arr\n",
    "\n",
    "def read_file_iterations(file):\n",
    "    '''\n",
    "    Args:\n",
    "    - file (file)            : filename\n",
    "\n",
    "    read the iterations from the file\n",
    "    '''\n",
    "    results = {}\n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for i, lines in enumerate(reader):\n",
    "            if lines[0] == '': continue\n",
    "\n",
    "            lines = lines[1:]\n",
    "\n",
    "            results[i] = [float(i) for i in lines]\n",
    "\n",
    "            # lines[1:]\n",
    "\n",
    "    # print(results)\n",
    "    # print(len(results[0]))\n",
    "    return results\n",
    "\n",
    "def change_dict(d, features):\n",
    "    '''\n",
    "    Args:\n",
    "    - d (dictionary)          : dictionary\n",
    "    - features (array)        : features of the dataset\n",
    "\n",
    "\n",
    "    get paths to store the results in\n",
    "    '''\n",
    "    arr = []\n",
    "    for row in d:\n",
    "        r = OrderedDict()\n",
    "        for f in features:\n",
    "            r[f] = row[f]\n",
    "        arr.append(r)\n",
    "\n",
    "    return arr\n",
    "\n",
    "def convert_new_df(d1, d2, path1, path2):\n",
    "    '''\n",
    "    Args:\n",
    "    - d1 (df)           : dataframe\n",
    "    - d2 (df)           : dataframe\n",
    "    - path1 (str)       : path of file\n",
    "    - path2 (str)       : path of file\n",
    "\n",
    "    convert to new dataframe\n",
    "    '''\n",
    "\n",
    "    df1 = pd.DataFrame(d1)\n",
    "    df1.to_csv(path1)\n",
    "\n",
    "    df2 = pd.DataFrame(d2)\n",
    "    df2.to_csv(path2)\n",
    "\n",
    "    return df1, df2\n",
    "\n",
    "def target_calculate_relevance_difference(params, y_train, y_test, pdiffs, reg, step, multiplier):\n",
    "    '''\n",
    "    Args:\n",
    "    - params (list[dict])   : parameters of the optimized y we found\n",
    "    - y_train (array)     : y train\n",
    "    - y_test (array)      : y test\n",
    "    - pdiffs (list[float])  : list of differences in true - predicted\n",
    "    - reg (model)           : regression model\n",
    "    - step (int)            : iterate every step size\n",
    "    - multiplier (int)      : multiplier target\n",
    "\n",
    "    Returns list[float] of differences between true and predicted y relevance for given optimizations\n",
    "    '''\n",
    "    difference_of_relevance = []\n",
    "    y_min = min(y_test)\n",
    "    y_max = max(y_test)\n",
    "\n",
    "\n",
    "    p_mult = 1 + multiplier\n",
    "    n_mult = 1 - multiplier\n",
    "\n",
    "    # for however cases we did\n",
    "    for i in range(len(pdiffs)):\n",
    "        # have to find the specific phi function for each one to predict\n",
    "        if y_test[i * step] < 0: mult = n_mult\n",
    "        else: mult = p_mult\n",
    "\n",
    "        y_target = y_test[i * step] * mult\n",
    "        ph, phis = get_phis_local(y_train, y_test, y_min, y_max, y_test[i * step], y_target)\n",
    "        # parameters for optimized y\n",
    "        x_param = [params[i]]\n",
    "        # optimal y we found\n",
    "        y_pred = reg.predict(x_param)[0]\n",
    "        # rel of optimal y\n",
    "        y_pred_rel = phi.phi(pd.Series(y_pred), phi_parms=ph)\n",
    "        # rel of true y\n",
    "        # y_true_rel = phis[i * step]\n",
    "        # y_rel_diff = abs(1 - phis[i * step])\n",
    "        # print(f\"original y = {y_test[i * step]}, original rel = {phis[i * step]}, optimal y = {y_pred}, optimal rel = {y_pred_rel[0]}\")\n",
    "\n",
    "        relevance_difference = y_pred_rel\n",
    "        # relevance_difference = -abs(y_pred_rel[0] - y_true_rel) / y_rel_diff\n",
    "        # print(f\"rel diff = {abs(y_pred_rel[0] - y_true_rel)}\")\n",
    "        # print(f\"rel pdiff = {relevance_difference}\")\n",
    "\n",
    "        # closer our predicted rel is, the better\n",
    "        # relevance_difference = -abs(y_pred_rel - y_true_rel)\n",
    "\n",
    "        difference_of_relevance.append(relevance_difference)\n",
    "\n",
    "    return difference_of_relevance\n",
    "\n",
    "def calculate_relevance_difference(params, pdiffs, reg, phis, ph, step):\n",
    "    '''\n",
    "    Args:\n",
    "    - params (list[dict])   : parameters of the optimized y we found\n",
    "    - pdiffs (list[float])  : list of differences in true - predicted\n",
    "    - reg (model)           : regression model\n",
    "    - phis (list[float])    : list of actual relevance for y_test\n",
    "    - ph (relevacnce func)  : relevance function used to create phis\n",
    "    - step (int)            : iterate every step size\n",
    "\n",
    "    Returns list[float] of differences between true and predicted y relevance for given optimizations\n",
    "    '''\n",
    "    difference_of_relevance = []\n",
    "\n",
    "    # for however cases we did\n",
    "    for i in range(len(pdiffs)):\n",
    "        # parameters for optimized y\n",
    "        x_param = [params[i]]\n",
    "        # optimal y we found\n",
    "        y_pred = reg.predict(x_param)[0]\n",
    "        # rel of optimal y\n",
    "        y_pred_rel = phi.phi(pd.Series(y_pred), phi_parms=ph)\n",
    "        # rel of true y\n",
    "        y_true_rel = phis[i * step]\n",
    "\n",
    "        relevance_difference = y_pred_rel\n",
    "        difference_of_relevance.append(relevance_difference[0])\n",
    "\n",
    "    return difference_of_relevance\n",
    "\n",
    "def target_calculate_y_difference(y_test, params, pdiffs, reg, step, multiplier):\n",
    "    '''\n",
    "    Args:\n",
    "    - y_test (list)         : list of all y values in test dataset\n",
    "    - params (list[dict])   : parameters of the optimized y we found\n",
    "    - pdiffs (list[float])  : list of differences in true - predicted\n",
    "    - reg (model)           : regression model\n",
    "    - step (int)            : iterate every step size\n",
    "    - multiplier (int)      : multiplier target\n",
    "\n",
    "    Returns list[float] of differences between true and predicted y values for given optimizations\n",
    "    '''\n",
    "    difference_of_y = []\n",
    "\n",
    "    p_mult = 1 + multiplier\n",
    "    n_mult = 1 - multiplier\n",
    "\n",
    "    # this is how many y's we ran for\n",
    "    for i in range(len(pdiffs)):\n",
    "        if y_test[i * step] < 0: mult = n_mult\n",
    "        else: mult = p_mult\n",
    "\n",
    "        # the optimized y parameter\n",
    "        x_param = [params[i]]\n",
    "\n",
    "        # predict the optimal y we found\n",
    "        y_pred = reg.predict(x_param)[0]\n",
    "        # get the original y\n",
    "        y_target = y_test[i * step] * mult\n",
    "        # difference of y\n",
    "        y_diff = abs(y_target - y_test[i * step])\n",
    "        if y_diff == 0: abs_diff = 0\n",
    "        else: abs_diff = -abs(y_pred - y_target) / y_diff\n",
    "        # ^^ should be what we use to optimize\n",
    "        # after we need to take the optimized cases and convert to actual diff\n",
    "\n",
    "        '''\n",
    "        ( 1 - 1.2 ) / 0.2       = -100%\n",
    "        vs.\n",
    "        (1.4 - 1.2 ) / 0.2      = 100%\n",
    "        '''\n",
    "\n",
    "\n",
    "        # print(f\"optimal y: {y_pred}, original y: {y_test[i * step]}, target y: {y_target}, y_diff: {y_diff}\")\n",
    "\n",
    "        difference_of_y.append(abs_diff)\n",
    "\n",
    "\n",
    "    # return\n",
    "    return difference_of_y\n",
    "\n",
    "def calculate_y_difference(y_test, params, pdiffs, reg, step):\n",
    "    '''\n",
    "    Args:\n",
    "    - y_test (list)         : list of all y values in test dataset\n",
    "    - params (list[dict])   : parameters of the optimized y we found\n",
    "    - pdiffs (list[float])  : list of differences in true - predicted\n",
    "    - reg (model)           : regression model\n",
    "    - step (int)            : iterate every step size\n",
    "\n",
    "    Returns list[float] of differences between true and predicted y values for given optimizations\n",
    "    '''\n",
    "    difference_of_y = []\n",
    "\n",
    "    # this is how many y's we ran for\n",
    "    for i in range(len(pdiffs)):\n",
    "        # the optimized y parameter\n",
    "        x_param = [params[i]]\n",
    "\n",
    "        # predict the optimal y we found\n",
    "        y_pred = reg.predict(x_param)[0]\n",
    "        # get the original y\n",
    "        y_orig = y_test[i * step]\n",
    "        # get the difference between the two\n",
    "        abs_diff = y_pred - y_orig\n",
    "        if y_orig == 0: abs_diff = 0\n",
    "        else: abs_diff = ((y_pred - y_orig) / abs(y_orig)) * 100\n",
    "\n",
    "        difference_of_y.append(abs_diff)\n",
    "\n",
    "    # return\n",
    "    return difference_of_y\n",
    "\n",
    "def target_calculate_y_percent_difference(y_test, params, pdiffs, reg, step, multiplier):\n",
    "    '''\n",
    "    Args:\n",
    "    - y_test (list)         : list of all y values in test dataset\n",
    "    - params (list[dict])   : parameters of the optimized y we found\n",
    "    - pdiffs (list[float])  : list of differences in true - predicted\n",
    "    - reg (model)           : regression model\n",
    "    - step (int)            : iterate every step size\n",
    "    - multiplier (int)      : multiplier target\n",
    "\n",
    "    Returns list[float] of differences between true and predicted y values for given optimizations\n",
    "    '''\n",
    "    percent_diff = []\n",
    "\n",
    "    p_mult = 1 + multiplier\n",
    "    n_mult = 1 - multiplier\n",
    "\n",
    "    # this is how many y's we ran for\n",
    "    for i in range(len(pdiffs)):\n",
    "        if y_test[i * step] < 0: mult = n_mult\n",
    "        else: mult = p_mult\n",
    "        # the optimized y parameter\n",
    "        x_param = [params[i]]\n",
    "        # predict the optimal y we found\n",
    "        y_pred = reg.predict(x_param)[0]\n",
    "        # get the original y\n",
    "        y_target = y_test[i * step] * mult\n",
    "        # difference of y\n",
    "        y_diff = abs(y_target - y_test[i * step])\n",
    "        if y_diff == 0: abs_diff = 0\n",
    "        else: abs_diff = (y_pred - y_target) / y_diff\n",
    "        # orig = 10, target = 20, y_pred = 7\n",
    "        # 7 - 20 / 10 = -0.3\n",
    "        # 19 - 20 / 10 = -0.1\n",
    "        # 21 - 20 / 10 = 0.1\n",
    "        percent_diff.append(abs_diff)\n",
    "\n",
    "    return percent_diff\n",
    "\n",
    "def calculate_distance(X_test, features_abs, features_rel, step):\n",
    "    '''\n",
    "    Args:\n",
    "    - X_test (list)         : X test data\n",
    "    - features_abs (list)   : features for rel diff\n",
    "    - features_rel (list)   : features for y diff\n",
    "    - step (int)            : iterate every step size\n",
    "\n",
    "    calculates distance between original feature and predicted feature\n",
    "    '''\n",
    "    params_orig = []\n",
    "\n",
    "    X_f = X_test.values.tolist()\n",
    "    # X_f_scaled = (X_f-np.min(X_f))/(np.max(X_f)-np.min(X_f))\n",
    "\n",
    "    # store original parameters\n",
    "    for i in range(0, len(X_f), step):\n",
    "        params_orig.append(X_f[i])\n",
    "\n",
    "    # set the optimized parameters\n",
    "    f1 = np.array(features_abs)\n",
    "    f2 = np.array(features_rel)\n",
    "\n",
    "    # find distances between the two\n",
    "    distance_abs = euclidean_distances(params_orig, f1)\n",
    "    distance_rel = euclidean_distances(params_orig, f2)\n",
    "\n",
    "    d_abs = []\n",
    "    d_rel = []\n",
    "\n",
    "    # find the mean distance between them\n",
    "    for d in distance_abs: d_abs.append(np.mean(d))\n",
    "    for d in distance_rel: d_rel.append(np.mean(d))\n",
    "\n",
    "\n",
    "    relative_diff = []\n",
    "    squared_diff = []\n",
    "    for i in range(len(d_rel)):\n",
    "        relative_diff.append(((d_rel[i] - d_abs[i]) / d_abs[i]) * 100)\n",
    "        squared_diff.append((d_rel[i] - d_abs[i]) * (d_rel[i] - d_abs[i]))\n",
    "\n",
    "    return distance_abs, distance_rel, squared_diff\n",
    "    # return relative_diff, squared_diff\n",
    "\n",
    "#####################   FOR RETRIEVING DATA + CALCULATING STATISTICS   #####################\n",
    "\n",
    "def process_results(names, models, file_intermediate, file_result, is_global, multiplier, step):\n",
    "    '''\n",
    "    Args:\n",
    "    - name (str)            : dataset name\n",
    "    - model (str)           : model to run\n",
    "    - file_intermediate     : intermediate filename\n",
    "    - file_result           : result filename\n",
    "    - is_global (bool)      : if we are optimizing for the maximum we can get or a specific target\n",
    "    - step (int)            : data point frequency usae\n",
    "    - multiplier (float)    : target multiplier\n",
    "\n",
    "    process all results\n",
    "    '''\n",
    "    # fields for csv results\n",
    "    if is_global: fields = ['Name', 'Model', 'Optimize For', 'Y Mean', 'Y SD', 'Relevance Mean', 'Relevance SD', 'Distance', 'Squared Distance']\n",
    "    else: fields = ['Name', 'Model', 'Optimize For', 'Y Mean', 'Y SD', 'Relevance Mean', 'Relevance SD', 'Iterations', 'Distance', 'Squared Distance']\n",
    "    \n",
    "    results = run_all_files(names, models, is_global, multiplier, step)\n",
    "    convert_results_to_csv(file_intermediate, results)\n",
    "    update_csv(fields, file_intermediate, file_result, is_global)\n",
    "\n",
    "    print(f'Finished converting results!\\n File is named: {file_result}')\n",
    "\n",
    "def run_all_files(names, models, is_global, multiplier, step):\n",
    "    '''\n",
    "    Args:\n",
    "    - names (list[str])     : names of datasets we want to run\n",
    "    - models (list[str])    : models (rf or lgbm)\n",
    "    - is_global (bool)      : if we are optimizing for the maximum we can get or a specific target\n",
    "    - multiplier (float)    : target multiplier\n",
    "    - step (int)            : data point frequency\n",
    "    \n",
    "    Runs 'run_file' on every file we have results for and returns in a list to convert into a csv\n",
    "    '''\n",
    "    results = []\n",
    "    for name in names:\n",
    "        print(f\"Running {name}.\")\n",
    "        for model in models:\n",
    "            print(f\"Running {model}.\")\n",
    "            results.append(run_file(name, model, is_global, multiplier, step))\n",
    "            print(f\"Finisihed running {name} with {model}.\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_file(name, model, is_global, multiplier, step):\n",
    "    '''\n",
    "    Args:\n",
    "    - name (str)    : dataset name\n",
    "    - model (str)   : model name (rf or lgbm)\n",
    "    - is_global (bool)      : if we are optimizing for the maximum we can get or a specific target\n",
    "    - multiplier (float)    : target multiplier\n",
    "    - step (int)            : data point frequency\n",
    "    \n",
    "    Run file --> get data from csv --> get statistics of interest --> return dictionary of the statistics\n",
    "    '''\n",
    "    X_train, y_train, X_test, y_test, ph, phis, reg, pdiffs_abs, pdiffs_rel, features_abs, features_rel, iterations_abs, iterations_rel, step = get_all_data(name, model, is_global, multiplier, step)\n",
    "\n",
    "    print('getting diff')\n",
    "\n",
    "    # getting relevance and y diff for the other data\n",
    "    if is_global:\n",
    "        difference_of_relevance_abs_y = calculate_relevance_difference(features_abs, pdiffs_abs, reg, phis, ph, step)\n",
    "        difference_of_relevance_abs_rel_y = calculate_relevance_difference(features_rel, pdiffs_rel, reg, phis, ph, step)\n",
    "        the_difference_of_y = calculate_y_difference(y_test, features_rel, pdiffs_rel, reg, step)\n",
    "        diff_y = calculate_y_difference(y_test, features_abs, pdiffs_abs, reg, step) #IS SAME AS PDIFFS_ABS\n",
    "    else:\n",
    "        difference_of_relevance_abs_y = target_calculate_relevance_difference(features_abs, y_train, y_test, pdiffs_abs, reg, step, multiplier)\n",
    "        difference_of_relevance_abs_rel_y = target_calculate_relevance_difference(features_rel, y_train, y_test, pdiffs_rel, reg, step, multiplier)\n",
    "        the_difference_of_y = target_calculate_y_percent_difference(y_test, features_rel, pdiffs_rel, reg, step, multiplier)\n",
    "        diff_y = target_calculate_y_percent_difference(y_test, features_abs, pdiffs_abs, reg, step, multiplier)\n",
    "        # the_difference_of_y = target_calculate_y_difference(y_test, features_rel, pdiffs_rel, reg, step, multiplier)\n",
    "        # diff_y = target_calculate_relevance_difference(features_abs, y_train, y_test, pdiffs_abs, reg, step, multiplier)\n",
    "\n",
    "\n",
    "\n",
    "    # if pdiffs_abs != diff_y: raise Exception('pdiffs_abs != diff_y')\n",
    "    # if pdiffs_rel != difference_of_relevance_abs_rel_y: raise Exception('pdiffs_rel != difference_of_relevance_abs')\n",
    "\n",
    "    if pdiffs_abs != diff_y: print(\"pdiffs_abs != diff_y\")\n",
    "    else: print(\"pdiffs_abs EQUAL to diff_y\")\n",
    "    # print(pdiffs_abs)\n",
    "    # print(diff_y)\n",
    "\n",
    "    print()\n",
    "\n",
    "    if pdiffs_rel != difference_of_relevance_abs_rel_y: print(\"pdiffs_rel != difference_of_relevance_abs_rel_y\")\n",
    "    else: print(\"pdiffs_rel EQUAL to difference_of_relevance_abs\")\n",
    "    # print(pdiffs_rel)\n",
    "    # print(difference_of_relevance_abs_rel_y)\n",
    "\n",
    "\n",
    "    print(\"\\nend\")\n",
    "\n",
    "    abs_distance, rel_distance, squared_dist = calculate_distance(X_test, features_abs, features_rel, step)\n",
    "\n",
    "    print('getting statistics')\n",
    "\n",
    "    # bo_y_median = np.median(pdiffs_abs)\n",
    "    # bo_y_stdev = np.std(pdiffs_abs)\n",
    "    # bor_y_median = np.median(the_difference_of_y)\n",
    "    # bor_y_stdev = np.std(the_difference_of_y)\n",
    "\n",
    "    # bo_rel_median = np.median(difference_of_relevance_abs_y)\n",
    "    # bo_rel_stdev = np.std(difference_of_relevance_abs_y)\n",
    "    # bor_rel_median = np.median(pdiffs_rel)\n",
    "    # bor_rel_stdev = np.std(pdiffs_rel)\n",
    "\n",
    "    if is_global:\n",
    "        bo_y_mean = np.mean(pdiffs_abs)\n",
    "        bo_y_stdev = np.std(pdiffs_abs)\n",
    "        bor_y_mean = np.mean(the_difference_of_y)\n",
    "        bor_y_stdev = np.std(the_difference_of_y)\n",
    "\n",
    "        bo_rel_mean = np.mean(difference_of_relevance_abs_y)\n",
    "        bo_rel_stdev = np.std(difference_of_relevance_abs_y)\n",
    "        bor_rel_mean = np.mean(pdiffs_rel)\n",
    "        bor_rel_stdev = np.std(pdiffs_rel)\n",
    "    else:\n",
    "        bo_y_mean = ((1 + np.mean(diff_y)) * 100) * (multiplier)\n",
    "        bo_y_stdev = np.std(diff_y)\n",
    "        bor_y_mean = ((1 + np.mean(the_difference_of_y)) * 100) * (multiplier)\n",
    "        bor_y_stdev = np.std(the_difference_of_y)\n",
    "\n",
    "        bo_rel_mean = ((np.mean(difference_of_relevance_abs_y)) * 100)\n",
    "        bo_rel_stdev = np.std(difference_of_relevance_abs_y)\n",
    "        bor_rel_mean = ((np.mean(pdiffs_rel)) * 100)\n",
    "        bor_rel_stdev = np.std(pdiffs_rel)\n",
    "\n",
    "        abs_iter, err = get_iterations_till_target(y_test, features_abs, iterations_abs, reg, step, multiplier)\n",
    "    # print(f\"abs erriter for {name} on {model} : {abs_iter}\")\n",
    "        abs_iter = np.mean(abs_iter)\n",
    "\n",
    "\n",
    "        rel_iter, err = get_iterations_till_target(y_test, features_rel, iterations_rel, reg, step, multiplier)\n",
    "   #  print(f\"rel erriter for {name} on {model} : {rel_iter}\")\n",
    "        rel_iter = np.mean(rel_iter)\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    bo_rel_median = np.median(pdiffs_rel)\n",
    "    bo_rel_stdev = np.std(pdiffs_rel)\n",
    "    bor_rel_median = np.median(difference_of_relevance_abs_y)\n",
    "    bor_rel_stdev = np.std(difference_of_relevance_abs_y)\n",
    "    '''\n",
    "    a_dist = np.mean(abs_distance)\n",
    "    r_dist = np.mean(rel_distance)\n",
    "    # p_dist = np.mean(percent_dist)\n",
    "    s_dist = np.mean(squared_dist)\n",
    "\n",
    "    # return {\n",
    "    #     \"name\": name,\n",
    "    #     \"model\": model,\n",
    "    #     \"bo_y_median\": bo_y_median,\n",
    "    #     \"bo_y_stdev\": bo_y_stdev,\n",
    "    #     \"bor_y_median\": bor_y_median,\n",
    "    #     \"bor_y_stdev\": bor_y_stdev,\n",
    "    #     \"bo_rel_median\": bo_rel_median,\n",
    "    #     \"bo_rel_stdev\": bo_rel_stdev,\n",
    "    #     \"bor_rel_median\": bor_rel_median,\n",
    "    #     \"bor_rel_stdev\": bor_rel_stdev,\n",
    "    #     \"percent_dist\": p_dist,\n",
    "    #     \"squared_dist\": s_dist\n",
    "    # }\n",
    "\n",
    "    if is_global:\n",
    "\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"model\": model,\n",
    "            \"bo_y_mean\": bo_y_mean,\n",
    "            \"bo_y_stdev\": bo_y_stdev,\n",
    "            \"bor_y_mean\": bor_y_mean,\n",
    "            \"bor_y_stdev\": bor_y_stdev,\n",
    "            \"bo_rel_mean\": bo_rel_mean,\n",
    "            \"bo_rel_stdev\": bo_rel_stdev,\n",
    "            \"bor_rel_mean\": bor_rel_mean,\n",
    "            \"bor_rel_stdev\": bor_rel_stdev,\n",
    "            \"abs_dist\": a_dist,\n",
    "            \"rel_dist\": r_dist,\n",
    "            \"squared_dist\": s_dist\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"model\": model,\n",
    "        \"bo_y_mean\": bo_y_mean,\n",
    "        \"bo_y_stdev\": bo_y_stdev,\n",
    "        \"bor_y_mean\": bor_y_mean,\n",
    "        \"bor_y_stdev\": bor_y_stdev,\n",
    "        \"bo_rel_mean\": bo_rel_mean,\n",
    "        \"bo_rel_stdev\": bo_rel_stdev,\n",
    "        \"bor_rel_mean\": bor_rel_mean,\n",
    "        \"bor_rel_stdev\": bor_rel_stdev,\n",
    "        \"abs_iter\": abs_iter,\n",
    "        \"rel_iter\": rel_iter,\n",
    "        \"abs_dist\": a_dist,\n",
    "        \"rel_dist\": r_dist,\n",
    "        \"squared_dist\": s_dist\n",
    "    }\n",
    "\n",
    "\n",
    "def get_all_data(name, model, is_global, multiplier, step):\n",
    "    '''\n",
    "    Args:\n",
    "    - name (str)    : name of dataset\n",
    "    - model (str)   : name of model\n",
    "    - is_global (bool)      : if we are optimizing for the maximum we can get or a specific target\n",
    "    - multiplier (float)    : target multiplier\n",
    "    - step (int)            : data point frequency\n",
    "\n",
    "    finds csv files that stored the results and reads data from them + dataset and return it for use\n",
    "    '''\n",
    "\n",
    "    X_train, y_train, X_test, y_test, features, ph, phis, reg = pre_process_data(name, model)\n",
    "\n",
    "    # get paths to csv for us to open\n",
    "    pdiffs_abs_file, pdiffs_rel_file, f_abs_file, f_rel_file, r_abs_file, r_rel_file = get_paths(name, model, is_global, multiplier)\n",
    "\n",
    "\n",
    "    # read data from files and store them\n",
    "    pdiffs_abs = read_file(pdiffs_abs_file)\n",
    "    pdiffs_rel = read_file(pdiffs_rel_file)\n",
    "    features_abs = read_file(f_abs_file)\n",
    "    features_rel = read_file(f_rel_file)\n",
    "    iterations_abs = read_file(r_abs_file)\n",
    "    iterations_rel = read_file(r_rel_file)\n",
    "\n",
    "    # step = round(len(X_test) / len(pdiffs_abs))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, ph, phis, reg, pdiffs_abs, pdiffs_rel, features_abs, features_rel, iterations_abs, iterations_rel, step\n",
    "\n",
    "def test_files(names):\n",
    "    '''\n",
    "    Args:\n",
    "    - names (list[str]) : names of datasets\n",
    "\n",
    "    Checks if the the datasets valid\n",
    "    For all dataset files (not including insurance.csv)\n",
    "    '''\n",
    "    for name in names:\n",
    "        if file_is_of_dataset(name): continue\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def file_is_of_dataset(name):\n",
    "    '''\n",
    "    Args:\n",
    "    - name (str) : name of file to check\n",
    "    checks if a file is part of the datasets with innate function to load it\n",
    "    '''\n",
    "    try: load_dataset(name)\n",
    "    except Exception: return False\n",
    "    return True\n",
    "\n",
    "def convert_results_to_csv(filename, results):\n",
    "    '''\n",
    "    Args:\n",
    "    - filename (str)        : name of file\n",
    "    - results (dictionary)  : results\n",
    "\n",
    "    converts the results to a csv so we can store it\n",
    "    '''\n",
    "    keys = results[0].keys()\n",
    "\n",
    "    with open(filename, 'w', newline='') as out:\n",
    "        dict_writer = csv.DictWriter(out, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(results)\n",
    "\n",
    "def update_csv(fields, file_intermediate, file_result, is_global):\n",
    "    '''\n",
    "    Args:\n",
    "    - fields (list[str])    : csv fields\n",
    "    - file_in (str)         : read file\n",
    "    - file_out (str)        : write file\n",
    "\n",
    "    update csv with new structure\n",
    "    '''\n",
    "    with open(file_result, 'w') as out:\n",
    "        writer = csv.DictWriter(out, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "\n",
    "\n",
    "    # open csv to read from\n",
    "        f = open(file_intermediate, \"r\")\n",
    "        for line in csv.DictReader(f):\n",
    "            for val in line:\n",
    "                try: line[val] = float(line[val])\n",
    "                except: continue\n",
    "\n",
    "            if is_global:\n",
    "                # FOR Y OPT\n",
    "                writer.writerow({\n",
    "                    'Name': line['name'],\n",
    "                    'Model': line['model'],\n",
    "                    'Optimize For': 'Y',\n",
    "                    'Y Mean': round(float(line['bo_y_mean']), 5),\n",
    "                    'Y SD': round(line['bo_y_stdev'], 5),\n",
    "                    'Relevance Mean': round(float(line['bo_rel_mean']), 5),\n",
    "                    'Relevance SD': round(float(line['bo_rel_stdev']), 5),\n",
    "                    'Distance': round(line['abs_dist'], 5),\n",
    "                    'Squared Distance': round(line['squared_dist'], 5),\n",
    "                })\n",
    "                # FOR REL OPT\n",
    "                writer.writerow({\n",
    "                    'Name': line['name'],\n",
    "                    'Model': line['model'],\n",
    "                    'Optimize For': 'Relevance',\n",
    "                    'Y Mean': round(line['bor_y_mean'], 5),\n",
    "                    'Y SD': round(line['bor_y_stdev'], 5),\n",
    "                    'Relevance Mean': round(line['bor_rel_mean'], 5),\n",
    "                    'Relevance SD': round(line['bor_rel_stdev'], 5),\n",
    "                    'Distance': round(line['rel_dist'], 5),\n",
    "                    'Squared Distance': round(line['squared_dist'], 5),\n",
    "                })\n",
    "\n",
    "            else:\n",
    "\n",
    "                # FOR Y OPT\n",
    "                writer.writerow({\n",
    "                    'Name': line['name'],\n",
    "                    'Model': line['model'],\n",
    "                    'Optimize For': 'Y',\n",
    "                    'Y Mean': round(float(line['bo_y_mean']), 5),\n",
    "                    'Y SD': round(line['bo_y_stdev'], 5),\n",
    "                    'Relevance Mean': round(float(line['bo_rel_mean']), 5),\n",
    "                    'Relevance SD': round(float(line['bo_rel_stdev']), 5),\n",
    "                    'Iterations': round(line['abs_iter'], 5),\n",
    "                    'Distance': round(line['abs_dist'], 5),\n",
    "                    'Squared Distance': round(line['squared_dist'], 5),\n",
    "                })\n",
    "                # FOR REL OPT\n",
    "                writer.writerow({\n",
    "                    'Name': line['name'],\n",
    "                    'Model': line['model'],\n",
    "                    'Optimize For': 'Relevance',\n",
    "                    'Y Mean': round(line['bor_y_mean'], 5),\n",
    "                    'Y SD': round(line['bor_y_stdev'], 5),\n",
    "                    'Relevance Mean': round(line['bor_rel_mean'], 5),\n",
    "                    'Relevance SD': round(line['bor_rel_stdev'], 5),\n",
    "                    'Iterations': round(line['rel_iter'], 5),\n",
    "                    'Distance': round(line['rel_dist'], 5),\n",
    "                    'Squared Distance': round(line['squared_dist'], 5),\n",
    "                })\n",
    "        f.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # print(run_file('insurance', 'rf'))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must be absolute path to a directory\n",
    "path = f\"/Users/[your user]/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'sex', 'bmi', 'children', 'smoker', 'region'], dtype='object')\n",
      "received data\n",
      "age, bmi, sex_female, sex_male, children_0, children_1, children_2, children_3, children_4, children_5, smoker_no, smoker_yes, region_northeast, region_northwest, region_southeast, region_southwest\n",
      "received phi/phis\n",
      "received regressor\n",
      "starting baysopt\n",
      "start abs bayesopt\n",
      "['age', 'bmi', 'sex_female', 'sex_male', 'children_0', 'children_1', 'children_2', 'children_3', 'children_4', 'children_5', 'smoker_no', 'smoker_yes', 'region_northeast', 'region_northwest', 'region_southeast', 'region_southwest']\n",
      "end abs bayesopt\n",
      "start rel bayesopt\n",
      "end rel bayesopt\n"
     ]
    }
   ],
   "source": [
    "name = 'insurance'\n",
    "is_global = False\n",
    "multiplier = 1.2\n",
    "step = 100\n",
    "models = ['rf', 'lgbm']\n",
    "get_algorithmic_recourse_results(name, models[0], step, is_global, multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running insurance.\n",
      "Running rf.\n",
      "Index(['age', 'sex', 'bmi', 'children', 'smoker', 'region'], dtype='object')\n",
      "received data\n",
      "age, bmi, sex_female, sex_male, children_0, children_1, children_2, children_3, children_4, children_5, smoker_no, smoker_yes, region_northeast, region_northwest, region_southeast, region_southwest\n",
      "received phi/phis\n",
      "received regressor\n",
      "getting diff\n",
      "pdiffs_abs EQUAL to diff_y\n",
      "\n",
      "pdiffs_rel EQUAL to difference_of_relevance_abs\n",
      "\n",
      "end\n",
      "getting statistics\n",
      "Finisihed running insurance with rf.\n",
      "Finished converting results!\n",
      " File is named: b\n"
     ]
    }
   ],
   "source": [
    "names = ['insurance']\n",
    "models = ['rf']\n",
    "is_global = True\n",
    "multipier = 1.2\n",
    "step = 100\n",
    "process_results(names, models, \"a\", \"b\", is_global, multiplier, step)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
